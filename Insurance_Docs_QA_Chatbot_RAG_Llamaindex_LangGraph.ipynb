{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sandy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sandy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%pip install langchain\n",
    "#%pip install langchain_openai\n",
    "\n",
    "# Import Necessary Libraries\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import Settings\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "\n",
    "import nest_asyncio\n",
    "from diskcache import Cache\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API key: Please Note this is a demo version, only openai is currently supported. Input is hidden\n"
     ]
    }
   ],
   "source": [
    "# Apply nested asyncio to allow for nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Ask user for OpenAI API key and set it\n",
    "print(\"Enter your OpenAI API key: Please Note this is a demo version, only openai is currently supported. Input is hidden\")\n",
    "api_key = getpass.getpass()\n",
    "openai.api_key = api_key\n",
    "\n",
    "# Initialize a persistent Chroma database client with the specified path\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma.db\")\n",
    "\n",
    "# Define the path to the directory containing PDF documents to be processed\n",
    "pdf_dir_path = \"./dataset_folder\"\n",
    "\n",
    "# Set the language model and embedding model to be used for processing\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "Settings.embed_model = OpenAIEmbedding()\n",
    "\n",
    "# Initialize a cache to store results, with a specified cache directory\n",
    "cache = Cache(\"./cache\")\n",
    "#Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(pdf_dir_path, storage_context):\n",
    "    \"\"\"Builds a vector store index from PDF documents.\n",
    "\n",
    "    Args:\n",
    "        pdf_dir_path (str): The path to the directory containing PDF documents to be indexed.\n",
    "        storage_context (StorageContext): The context for storing the vector index.\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex: The created vector store index containing the processed documents.\n",
    "    \"\"\"  \n",
    "    # Load documents from the specified directory\n",
    "    docs = SimpleDirectoryReader(pdf_dir_path).load_data()\n",
    "    \n",
    "    # Create an ingestion pipeline with specified transformations\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            # Split documents into sentences with specified chunk size and overlap\n",
    "            SentenceSplitter(chunk_size=2048, chunk_overlap=0),\n",
    "            # Extract titles from the documents\n",
    "            TitleExtractor(),\n",
    "            # Use OpenAI's embedding model for text embedding\n",
    "            OpenAIEmbedding(model_name=\"text-embedding-ada-002\"),\n",
    "            #HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Run the pipeline on the loaded documents to create nodes\n",
    "    nodes = pipeline.run(documents=docs)\n",
    "    \n",
    "    # Create a vector store index from the nodes and the provided storage context\n",
    "    index = VectorStoreIndex(nodes=nodes, storage_context=storage_context)\n",
    "\n",
    "    return index  # Return the created index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_retrieval(query, index):\n",
    "    \"\"\"Retrieves data based on the provided query from the specified index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string used to search for relevant data.\n",
    "        index (VectorStoreIndex): The index from which to retrieve data.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of results matching the query.\n",
    "    \"\"\"\n",
    "    # Convert the index into a retriever object for querying\n",
    "    retriever = index.as_retriever()\n",
    "    \n",
    "    # Retrieve results based on the provided query\n",
    "    results = retriever.retrieve(query)\n",
    "    \n",
    "    return results  # Return the retrieved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_index():\n",
    "    \"\"\"Creates and saves a vector store index from PDF documents.\n",
    "\n",
    "    This function attempts to create a new Chroma collection for storing the index.\n",
    "    If the collection already exists, it retrieves the existing collection.\n",
    "    It then builds the index from the documents in the specified directory and saves it.\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex or None: The created vector store index if successful, otherwise None.\n",
    "    \"\"\"\n",
    "    print(\"Creating and saving the index\")\n",
    "    \n",
    "    # Attempt to create a new Chroma collection for storing the index\n",
    "    try:\n",
    "        chroma_collection = chroma_client.create_collection(name=\"Insurance_Doc_RAG_LlamaIndex_LangGraph\")\n",
    "    except Exception as e:\n",
    "        # If the collection already exists, retrieve the existing collection\n",
    "        print(f\"Collection already exists: {e}\")\n",
    "        chroma_collection = chroma_client.get_collection(name=\"Insurance_Doc_RAG_LlamaIndex_LangGraph\")\n",
    "\n",
    "    # Initialize a vector store with the Chroma collection\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    \n",
    "    # Create a storage context using the default settings and the vector store\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    # Attempt to build the index from the documents and save it\n",
    "    try:\n",
    "        index = build_index(pdf_dir_path, storage_context)\n",
    "        print(\"Index created and saved\")\n",
    "        return index  # Return the created index\n",
    "    except Exception as e:\n",
    "        # If an error occurs during index building, print the error and return None\n",
    "        print(f\"Error while building the index: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index():\n",
    "    \"\"\"Loads the vector store index from a Chroma collection.\n",
    "\n",
    "    This function attempts to load a vector store index from a specified Chroma collection.\n",
    "    If the collection exists, it retrieves the collection and constructs a vector store index.\n",
    "    If the collection does not exist or an error occurs, it returns None.\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex or None: The loaded vector store index if successful, otherwise None.\n",
    "    \"\"\"\n",
    "    # Attempt to retrieve the specified collection from the Chroma database\n",
    "    try:\n",
    "        chroma_collection = chroma_client.get_collection(name=\"Insurance_Doc_RAG_LlamaIndex_LangGraph\")\n",
    "    except Exception as e:\n",
    "        # Print an error message if the collection cannot be loaded\n",
    "        print(f\"Error loading the collection: {e}\")\n",
    "        return None\n",
    "\n",
    "    print(\"Loading the index\")\n",
    "    # Initialize a ChromaVectorStore with the retrieved collection\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    # Create a storage context using default settings and the vector store\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    try:\n",
    "        # Attempt to create a VectorStoreIndex from the vector store and storage context\n",
    "        index = VectorStoreIndex.from_vector_store(\n",
    "            vector_store=vector_store,\n",
    "            storage_context=storage_context\n",
    "        )\n",
    "        return index  # Return the successfully loaded index\n",
    "    except Exception as e:\n",
    "        # Print an error message if the index cannot be loaded\n",
    "        print(f\"Error while loading the index: {e}\")\n",
    "        return None  # Return None if the index cannot be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading the collection: Collection Insurance_Doc_RAG_LlamaIndex_LangGraph does not exist.\n",
      "Creating and saving the index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created and saved\n",
      "Index is ready\n"
     ]
    }
   ],
   "source": [
    "# Attempt to load the existing vector store index\n",
    "index = load_index()\n",
    "\n",
    "# If loading the index fails, attempt to create and save a new index\n",
    "if index is None:\n",
    "    index = save_index()\n",
    "\n",
    "# Check if the index is successfully loaded or created\n",
    "if index:\n",
    "    print(\"Index is ready\")  # Indicate that the index is ready for use\n",
    "else:\n",
    "    print(\"Failed to create or load the index\")  # Indicate failure in loading or creating the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_docs(query):\n",
    "    \"\"\"Retrieves documents based on the provided query, utilizing a cache for efficiency.\n",
    "\n",
    "    This function first checks if the results for the given query are already cached. \n",
    "    If cached results are found, they are returned immediately. \n",
    "    If not, it retrieves the results from the index and caches them for future use.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string used to search for relevant documents.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of results matching the query.\n",
    "    \"\"\"\n",
    "    # Check if the results for the query are already in the cache\n",
    "    if cache.get(query) is not None:\n",
    "        return cache.get(query)  # Return cached results if available\n",
    "    \n",
    "    # Retrieve results from the index if not cached\n",
    "    results = data_retrieval(query, index)\n",
    "    \n",
    "    # Store the retrieved results in the cache with a specified expiration time\n",
    "    cache.set(query, results, expire=600)\n",
    "\n",
    "    return results  # Return the retrieved results\n",
    "\n",
    "# Uncomment the following line to test the function with a sample query\n",
    "# res = retrive_docs(\"what is Waiting Period and Exclusions?\")\n",
    "# print(res)  # Uncomment to print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search_tool(query: str):\n",
    "    \"\"\"BACKUP TOOL - Only use this tool if docs_tool fails to provide relevant information \n",
    "    or returns incomplete results. This tool searches external sources.\"\"\"\n",
    "    # Use the SerpAPIWrapper to perform a search on external sources using the provided query\n",
    "    return SerpAPIWrapper(serpapi_api_key =\"663d549846fac3c10e2b7d0dfeed509b0923c171084ecf2b8b4574bef5b3683a\").run(query)\n",
    "\n",
    "@tool\n",
    "def docs_tool(query: str):\n",
    "    \"\"\"PREFERRED TOOL - Use this tool FIRST to search internal documentation and get information. \n",
    "    Only use other tools if this tool fails to provide relevant information.\"\"\"\n",
    "    # Retrieve documents based on the query using the internal retrieval function\n",
    "    return retrive_docs(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tools to be used, with docs_tool as the primary and search_tool as the backup\n",
    "tools = [docs_tool, search_tool]\n",
    "\n",
    "# Create a ToolNode that manages the defined tools\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# Initialize the language model with the specified model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1, api_key=api_key)\n",
    "\n",
    "# Bind the tools to the language model for enhanced functionality\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please Note - The following piece of code is not required to run the application and could/is used for internal understanding, alternate\n",
    "# prototyping ways and testing/debugging purposes. This code is not required to run this application which the code used above/below has superseded. \n",
    "\n",
    "# from typing import Annotated, List\n",
    "# from typing_extensions import TypedDict\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langgraph.graph import StateGraph, START, END\n",
    "# from langgraph.graph.message import add_messages\n",
    "# from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# class State(TypedDict):\n",
    "#     messages: List[dict]\n",
    "\n",
    "# graph_builder = StateGraph(State)\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# def chatbot(state: State):\n",
    "#     return {\"messages\" : [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "# tool_node = ToolNode(\n",
    "#     tools =[docs_tool, search_tool]\n",
    "# )\n",
    "\n",
    "# graph_builder.add_node(\"chatbot\", chatbot)\n",
    "# graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "# graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "# graph_builder.add_conditional_edges(\"chatbot\", tools_condition, \"tools\")\n",
    "\n",
    "# graph_builder.add_edge(START, \"chatbot\")\n",
    "# graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "# graph = graph_builder.compile()\n",
    "\n",
    "# llm_with_tools.invoke(\"What is the procedure to claim the insurance?\").tool_calls\n",
    "\n",
    "# tool_node.invoke({\"messages\": [llm_with_tools.invoke(\"What is the procedure to claim the insurance?\")]})\n",
    "\n",
    "# for chunk in app.stream(\n",
    "#     {\"messages\": [(\"human\", \"what is Accelerated Critical Illness Benefit in the Insurance Doc?\")]}, stream_mode=\"values\"\n",
    "# ):\n",
    "#     chunk[\"messages\"][-1].pretty_print()\n",
    "# data = []\n",
    "\n",
    "# for chunk in app.stream(\n",
    "#     {\"messages\": [(\"human\", \"what is Accelerated Critical Illness Benefit in the Insurance Doc?\")]}, stream_mode=\"values\"\n",
    "# ):\n",
    "#     data.append(chunk[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: MessagesState):\n",
    "    \"\"\"Determines whether the conversation should continue based on the last message.\n",
    "\n",
    "    This function checks the last message in the conversation state to see if it contains any tool calls.\n",
    "    If tool calls are present, it indicates that the conversation should continue with tool usage.\n",
    "    Otherwise, it signals the end of the conversation.\n",
    "\n",
    "    Args:\n",
    "        state (MessagesState): The current state of the conversation, including messages.\n",
    "\n",
    "    Returns:\n",
    "        str: \"tools\" if there are tool calls in the last message, otherwise returns END.\n",
    "    \"\"\"\n",
    "    # Extract the list of messages from the conversation state\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Get the last message in the conversation\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Check if the last message contains any tool calls\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"  # Indicate that the conversation should continue with tools\n",
    "    \n",
    "    return END  # Indicate that the conversation should end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState):\n",
    "    \"\"\"Invokes the language model with the current conversation messages.\n",
    "\n",
    "    This function takes the current state of the conversation, extracts the messages,\n",
    "    and passes them to the language model for processing. It returns the model's response\n",
    "    in a structured format.\n",
    "\n",
    "    Args:\n",
    "        state (MessagesState): The current state of the conversation, including messages.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the model's response wrapped in a list under the key \"messages\".\n",
    "    \"\"\"\n",
    "    # Extract the list of messages from the conversation state\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    #print(messages)\n",
    "    # Add a prompt to the messages.\n",
    "    prompt = \"\"\"\n",
    "    Give the reference of the document i.e page number and document name if you got the answer from the document. \n",
    "    If you got the answer from internet search, tell the same.\n",
    "    The response format should be:\n",
    "    <Answer>\n",
    "    <Reference>\n",
    "    reference format : Page Number | Page Title | Document Name\n",
    "    Example: Page 16 | Comprehensive Guide to Insurance Policy Administration: Managing Claims, Eligibility, and Member Information Requirements | HDFC-Life-Group-Poorna-Suraksha-101N137V02-Policy-Document.pdf\n",
    "    if you got the answer from internet search, Say \"Answer retrieved from internet search. Please contact the Agent/Customer Care executive for more details\" in Reference.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Invoke the language model with the current messages to get a response\n",
    "\n",
    "    from langchain_core.messages import HumanMessage\n",
    "\n",
    "    messages.append(HumanMessage(content=prompt))\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Return the response in a structured format\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a state graph to manage the flow of the conversation\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Add a node for the agent that will handle the conversation logic\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "\n",
    "# Add a node for the tools that can be used during the conversation\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Define the starting point of the workflow, connecting the START node to the agent\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Add conditional edges to determine the next step based on the agent's response\n",
    "# If the agent's response indicates tool usage, transition to the tools node; otherwise, end the conversation\n",
    "workflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\n",
    "\n",
    "# Connect the tools node back to the agent to allow for repeated tool usage\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compile the workflow into an executable application\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the workflow graph as a Mermaid diagram using the application's graph representation\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please Note - The following piece of code is not required to run the application and is used for internal understanding, alternate\n",
    "# prototyping ways and testing/debugging purposes. This code is not required to run this application which the code used above/below has superseded. \n",
    "\n",
    "# def stream_graph_updates(user_input: str):\n",
    "#     for event in graph.stream({\"messages\": [(\"user\", user_input)]}):\n",
    "#         for value in event.values():\n",
    "#             print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "# while True:\n",
    "#     user_input = input(\"User: \")\n",
    "#     if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "#         print(\"Goodbye!\")\n",
    "#         break\n",
    "\n",
    "#     stream_graph_updates(user_input)\n",
    "\n",
    "# print(\"Welcome to Insurance Documentation Chatbot. Please enter your query. Type 'exit' to quit.\")\n",
    "# chat_history = []\n",
    "# query = input()\n",
    "# print(\"User: \", query)\n",
    "# while query != 'exit':\n",
    "#     response = agent.invoke({\"input\": query})\n",
    "#     #chat_history.append({\"input\": query, \"output\": response['output']})\n",
    "#     #cache.set(response['input'], response['output'], expire=600)\n",
    "#     print(\"Chatbot: \", response['output'])\n",
    "#     query = input()\n",
    "#     print(\"User: \", query)\n",
    "\n",
    "# if 'exit' in query.lower():\n",
    "#     print(\"Thanks for using Insurance Documentation Chatbot. Have a great day!\")\n",
    "\n",
    "\n",
    "\n",
    "# #agent.run(\"What is the procedure to claim the insurance?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a thread ID to the configuration for saving the conversation.\n",
    "# This ensures that each conversation is saved and context is retained.\n",
    "\n",
    "config = {\"configurable\" : {\"thread_id\" : \"2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Insurance Documentation Chatbot. Please enter your query. Type 'exit' to quit.\n",
      "User:  is Alzheimer's Disease covered under critical illness benefits?\n",
      "Searching... Please wait!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Chatbot:  Yes, Alzheimer's Disease is covered under critical illness benefits as specified in the policy document.\n",
      "\n",
      "Page 28 | Comprehensive Guide to Critical Medical Conditions and Their Definitions for Insurance Coverage | HDFC-Life-Group-Poorna-Suraksha-101N137V02-Policy-Document.pdf\n",
      "----------------------------------------------------------------------------------------------------\n",
      "User:  What is a Grievance Redressal Process?\n",
      "Searching... Please wait!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Chatbot:  The Grievance Redressal Process is a structured procedure that allows policyholders to address their complaints or grievances regarding insurance services. It typically involves the following steps:\n",
      "\n",
      "1. **Contacting the Grievance Redressal Officer**: Policyholders can reach out to the designated officer via a specified address, helpline number, or email.\n",
      "2. **Acknowledgment of Complaints**: Upon receiving a complaint, the insurer must acknowledge it within a certain timeframe, usually within 3 working days.\n",
      "3. **Investigation**: The insurer may investigate the complaint by gathering information from the policyholder.\n",
      "4. **Resolution Communication**: The insurer will communicate the resolution or rejection of the complaint, providing reasons for the decision.\n",
      "5. **Escalation**: If the policyholder is not satisfied with the response, they can escalate the matter to higher authorities within the insurance company or approach external bodies like the Insurance Ombudsman or the IRDAI.\n",
      "\n",
      "Page 19 | Comprehensive Guide to Grievance Redressal and Escalation Procedures for HDFC Life Insurance Policyholders | HDFC-Life-Group-Poorna-Suraksha-101N137V02-Policy-Document.pdf\n",
      "----------------------------------------------------------------------------------------------------\n",
      "User:  what is Waiting Period and Exclusions?\n",
      "Searching... Please wait!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Chatbot:  A waiting period is the amount of time an insured must wait before some or all of their coverage comes into effect. The insured may not receive benefits for claims filed during the waiting period. Exclusions refer to specific conditions or circumstances that are not covered by the insurance policy, meaning that the insurer will not pay for claims arising from these situations.\n",
      "\n",
      "Page 14 | Comprehensive Overview of Waiting Periods, Exclusions, and Permanent Exclusions for Accelerated Critical Illness Benefits | HDFC-Life-Group-Poorna-Suraksha-101N137V02-Policy-Document.pdf\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Thanks for using Insurance Documentation Chatbot. Have a great day!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Welcome message for the user and instructions on how to exit the chatbot\n",
    "print(\"Welcome to Insurance Documentation Chatbot. Please enter your query. Type 'exit' to quit.\")\n",
    "\n",
    "# Get the user's input query and print it to the Output cell\n",
    "query = input()\n",
    "\n",
    "# Continue the conversation until the user types 'exit'\n",
    "while query != 'exit':\n",
    "    print(\"User: \", query)\n",
    "    print(\"Searching... Please wait!\")\n",
    "    print(\"-\"*100)\n",
    "    data = []  # Initialize a list to store the chatbot's responses\n",
    "    \n",
    "    # Stream the chatbot's responses based on the user's query\n",
    "    for chunk in app.stream({\"messages\": [(\"human\", query)]}, stream_mode=\"values\", config=config):\n",
    "        #print(chunk)  # Print the raw response chunk. Uncomment this for debugging\n",
    "        data.append(chunk[\"messages\"][-1].content)  # Append the chatbot's response to the data list\n",
    "    \n",
    "    # Print the last response from the chatbot\n",
    "    print(\"Chatbot: \", data[-1])\n",
    "\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    # Get the next query from the user\n",
    "    query = input()\n",
    "    \n",
    "\n",
    "# Thank the user for using the chatbot when they exit\n",
    "if 'exit' in query.lower():\n",
    "    print(\"Thanks for using Insurance Documentation Chatbot. Have a great day!\")\n",
    "    print(\"-\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
